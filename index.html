<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>

  <title>Yunhai Han</title>
  
  <meta name="author" content="Yunhai Han">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yunhai Han</name>
              </p>
              <p>Welcome! My name is Yunhai Han and I am a Robotics PhD student at Georgia Institute of Technology, advised under <em><a href="https://harishravichandar.com/">Prof. Harish Ravichandar</a></em>. My research focus at Gatech is about structured robot learning. I am also honoured to be an awardee of Robotics PhD fellowship from <em><a href="https://research.gatech.edu/robotics">Georgia Tech's Institute for Robotics and Intelligent Machines (IRIM)</a></em>. Before coming to Georgia Tech, I received my M.S. / B.S. in Mechanical Engineering from UCSD, 2021 and Yanshan University, 2019, respectively. I'm open to discussions and collaborations, so feel free to drop me an email if you are interested.
                </p>
              
              <p style="text-align:center">
                <a href="mailto:njyunhai@gmail.com">Email</a> &nbsp/&nbsp
                <a href="CV_latest.pdf">CV(Aug 2025)</a> &nbsp/&nbsp
                <a href="https://github.com/y8han">Github</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=lsN3nY8AAAAJ&hl=en&authuser=1">Google Scholar</a> &nbsp/&nbsp
                <a href="http://linkedin.com/in/Yunhai-Han">Linkdin</a>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:80%;max-width:80%" alt="profile photo" src="images/profile_picture_2.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p> I have a broad interest in robotics, especially robot learning for manipulation tasks under complex environments, i.e., at home. 
                I enjoy developing algorithms for robotic applications and I would like to explore how robots could learn and perceive the world to achieve high-level automation skills in dynamical environments. 
	      </p>
	      <p>My love to robotics stems from a famous Japanese anime series: <em><a href="https://en.wikipedia.org/wiki/Code_Geass">Code Geass(コードギアス 反逆のルルーシュ)</a></em>. I was dreaming of building a macha when I was a high school student.
              </p>
            </td>
          </tr>
        </tbody></table>
        
        
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Mentored students</heading>
              <p> <a href="https://colinyu1.github.io/">Kelin Yu</a> (CS M.S., 2022-2024, now PhD at UMD CS) <br>
              <a href="https://chenzheny.github.io/">Zhenyang Chen</a> (Robotics M.S., 2023-2024, now PhD at GT Robotics) <br>
              <a href="https://www.linkedin.com/in/hanyao-guo-7422ba28a/">Hanyao Guo</a> (Robotics M.S., 2024-2025, now SDE at Amazon) <br>
              <a href="https://www.linkedin.com/in/pratik-shah-5a6a18136/">Pratik Shah</a> (CS M.S., 2024-) <br>
              <a href="https://www.linkedin.com/in/linhaobai/">Linhao Bai</a> (CS M.S., 2024-) <br>
              <a href="https://www.linkedin.com/in/%E7%BE%BD%E8%8C%9C-%E9%83%91-664159288/?locale=en_US">Yuqian Zheng</a> (CSE M.S., 2024-) <br>
              <a href="https://www.linkedin.com/in/ziyu-xiao-2a25a828a/">Ziyu Xiao</a> (ECE M.S., 2025-) <br>
              <a href="https://scholar.google.com/citations?user=clP6kVEAAAAJ&hl=en">Jianuo Qiu</a> (Robotics M.S., 2026-) <br>
	      </p>
            </td>
          </tr>
        </tbody></table>
        
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>External Collaborators (Past & Present)</heading>
              <p> <a href="https://jhkim.me/">Jeonghwan Kim</a> (GT CS PhD in Prof. Sehoon Ha's lab) <br>
              <a href="https://hychen-naza.github.io/">Hongyi Chen</a> (CMU Robotics PhD in Prof. Jeffrey Ichnowski's lab) <br>
              <a href="https://yangcenliu.github.io/homepage.github.io/">Yangcen Liu</a> (GT Robotics MS in Prof. Danfei Xu's lab) <br>
              <a href="https://www.linkedin.com/in/woochulshin/">Woochul Shin</a> (GT CS MS in Prof. Danfei Xu's lab) <br>
              <a href="https://manishanatarajan.github.io/">Manisha Natarajan</a> (GT Robotics PhD in Prof. Matthew Gombolay's lab) <br>
              <a href="https://zubairirshad.com/">Muhammad Zubair Irshad</a> (GT Robotics PhD in Prof. Zsolt Kira's lab) <br>
              <a href="https://kennyshaw.net/">Kenneth Shaw</a> (CMU Robotics PhD in Prof. Deepak Pathak's lab) <br>
              <a href="https://sizhewei.github.io/">Sizhe Wei</a> (GT Robotics PhD in Prof. Lu Gan's lab) <br>
	      </p>
            </td>
          </tr>
        </tbody></table>
        
         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publication</heading>
            </td>
            </tr>
        </tbody></table>
        <strong>&#160 &#160 &#160 2026</strong><br>
       
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="KUBM" onmouseover="KUBM">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ff_image'>
                  <img src='images/Unified_Behavioral_Models.png' width="180"></div>
                <img src='images/Unified_Behavioral_Models.png' width="180">
              </div>
              <script type="text/javascript">
                function superdeep_start() {
                  document.getElementById('ff_image').style.opacity = "1";
                }

                function superdeep_stop() {
                  document.getElementById('ff_image').style.opacity = "0";
                }
                superdeep_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
             <papertitle>Going with the Flow: Koopman Behavioral Models as Implicit Planners for Visuo-Motor Dexterity
                </papertitle>
              </a>
              <br>
	      <b>Yunhai Han</b>, Linhao Bai*, Ziyu Xiao*, Zhaodong Yang*, Yogita Choudhary, Krishna Jha, Chuizheng Kong, Shreyas Kousik, Harish Ravichandar (* Equal contribution)
              <br>
               <em>Under review</em>
              <br>
              There has been rapid and dramatic progress in robots' ability to learn complex visuo-motor manipulation skills from demonstrations, thanks in part to expressive policy classes that employ diffusion- and transformer-based backbones. However, these design choices require significant data and computational resources and remain far from reliable, particularly within the context of multi-fingered dexterous manipulation. Fundamentally, they model skills as reactive mappings and rely on fixed-horizon action chunking to mitigate jitter, creating a rigid trade-off between temporal coherence and reactivity. In this work, we introduce Unified Behavioral Models (UBMs), a framework that learns to represent dexterous skills as coupled dynamical systems that capture how visual features of the environment (visual flow) and proprioceptive states of the robot (action flow) co-evolve. By capturing such behavioral dynamics, UBMs can ensure temporal coherence by construction rather than by heuristic averaging. To operationalize these models, we propose Koopman-UBM, a first instantiation of UBMs that leverages Koopman Operator theory to effectively learn a unified representation in which the joint flow of latent visual and proprioceptive features is governed by a structured linear system. We demonstrate that Koopman-UBM can be viewed as an implicit planner: given an initial condition, it analytically computes the desired robot behavior while simultaneously ``imagining" the resulting flow of visual features over the entire skill horizon. 
              <br>
              [<a href="https://arxiv.org/pdf/2602.07413">arXiv</a>]
              <p></p>
            </td>
          </tr> 
          </table>
          
        <strong>&#160 &#160 &#160 2025</strong><br>
        
                                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="KoopLoco" onmouseover="KoopLoco">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ff_image'>
                  <img src='images/kooploco.png' width="180"></div>
                <img src='images/kooploco.png' width="180">
              </div>
              <script type="text/javascript">
                function superdeep_start() {
                  document.getElementById('ff_image').style.opacity = "1";
                }

                function superdeep_stop() {
                  document.getElementById('ff_image').style.opacity = "0";
                }
                superdeep_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
             <papertitle>Learning Koopman Dynamics for Safe Legged Locomotion with Reinforcement Learning-based Controller
                </papertitle>
              </a>
              <br>
	      Jeonghwan Kim, <b>Yunhai Han</b>, Harish Ravichandar, Sehoon Ha
              <br>
               <em>Under review</em>
              <br>
              Nonlinearity in dynamics has long been a major challenge in robotics, often causing significant performance degradation in existing control algorithms. For example, the navigation of bipedal robots can exhibit nonlinear behaviors even under simple velocity commands, as their actual dynamics are governed by complex whole-body movements and discrete contacts. In this work, we propose a novel safe navigation framework inspired by Koopman operator theory. We first train a low-level locomotion policy using deep reinforcement learning, and then capture its low-frequency, base-level dynamics by learning linearized dynamics in a high-dimensional lifted space using Dynamic Mode Decomposition. Then, our model-predictive controller (MPC) efficiently optimizes control signals via standard quadratic objective and the linear dynamics constraint in the lifted space. We demonstrate that the Koopman-based model more accurately predicts bipedal robot trajectories than baseline approaches. Furthermore, we show that the proposed navigation framework achieves improved safety with better success rates in dense environments with narrow passages.
              <br>
              [<a href="https://arxiv.org/abs/2409.14736">arXiv</a>][<a href="https://jhkim.me/kooploco/">Webpage</a>]
              <p></p>
            </td>
          </tr> 
          </table>
          
                                                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="Spec-clip" onmouseover="Spec-clip">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ff_image'>
                  <img src='images/IMmimic.png' width="180"></div>
                <img src='images/IMmimic.png' width="180">
              </div>
              <script type="text/javascript">
                function superdeep_start() {
                  document.getElementById('ff_image').style.opacity = "1";
                }

                function superdeep_stop() {
                  document.getElementById('ff_image').style.opacity = "0";
                }
                superdeep_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
             <papertitle>ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation
                </papertitle>
              </a>
              <br>
              Yangcen Liu*, Woochul Shin*, <b>Yunhai Han</b>, Zhenyang Chen, Harish Ravichandar, Danfei Xu (* Equal contribution)
              <br>
		<em>Accepted by CoRL 2025 (<b>Oral</b>) & Spotlight at RSS Dex Workshop </em>
              <br>
		Learning robot manipulation from abundant human videos offers a scalable alternative to costly robot-specific data collection. However, domain gaps across visual, morphological, and physical aspects hinder direct imitation. To effectively bridge the domain gap, we propose ImMimic, an embodiment-agnostic co-training framework that leverages both human videos and a small amount of teleoperated robot demonstrations. ImMimic uses Dynamic Time Warping (DTW) with either action- or visual-based mapping to map retargeted human hand poses to robot joints, followed by MixUp interpolation between paired human and robot trajectories. Our key insights are (1) retargeted human hand trajectories provide informative action labels, and (2) interpolation over the mapped data creates intermediate domains that facilitate smooth domain adaptation during co-training. Evaluations on four real-world manipulation tasks (Pick and Place, Push, Hammer, Flip) across four robotic embodiments (Robotiq, Fin Ray, Allegro, Ability) show that ImMimic improves task success rates and execution smoothness, highlighting its efficacy to bridge the domain gap for robust robot manipulation.
              <br>
               [<a href="https://arxiv.org/abs/2509.10952">Arxiv</a>][<a href="https://sites.google.com/view/immimic">Webpage</a>]
              <p></p>
            </td>
          </tr> 
          </table>
          
                                                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="Spec-clip" onmouseover="Spec-clip">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ff_image'>
                  <img src='images/Spectal_clipping-Geometric_intution.png' width="180"></div>
                <img src='images/Spectal_clipping-Geometric_intution.png' width="180">
              </div>
              <script type="text/javascript">
                function superdeep_start() {
                  document.getElementById('ff_image').style.opacity = "1";
                }

                function superdeep_stop() {
                  document.getElementById('ff_image').style.opacity = "0";
                }
                superdeep_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
             <papertitle>On the Surprising Effectiveness of Spectral Clipping in Learning Stable Linear and Latent-Linear Dynamical Systems
                </papertitle>
              </a>
              <br>
	      Hanyao Guo*, <b>Yunhai Han*</b>, Harish Ravichandar (* Equal contribution; Guo is my master student advisee)
              <br>
		<em>Under review</em>
              <br>
		When learning stable linear dynamical systems from data, three important properties are desirable: i) predictive accuracy, ii) verifiable stability, and iii) computational efficiency. Unconstrained minimization of prediction errors leads to high accuracy and efficiency but cannot guarantee stability. Existing methods to enforce stability often preserve accuracy, but do so only at the cost of increased computation. In this work, we investigate if a seemingly-naive procedure can simultaneously offer all three desiderata. Specifically, we consider a post-hoc procedure in which we surgically manipulate the spectrum of the linear system after it was learned using unconstrained least squares. We call this approach spectral clipping (SC) as it involves eigen decomposition and subsequent reconstruction of the system matrix after any eigenvalues whose magnitude exceeds one have been clipped to one (without altering the eigenvectors). We also show that SC can be readily combined with Koopman operators to learn nonlinear dynamical systems that can generate stable predictions of nonlinear phenomena, such as those underlying complex dexterous manipulation skills involving multi-fingered robotic hands. Through comprehensive experiments involving two different applications and publicly available benchmark datasets, we show that this simple technique can efficiently learn highly-accurate predictive dynamics that are provably-stable. Notably, we find that SC can match or outperform strong baselines while being orders-of-magnitude faster. Finally, we find that SC can learn stable robot policies even when the training data includes unsuccessful or truncated demonstrations.
              <br>
               [<a href="https://arxiv.org/abs/2412.01168">Arxiv</a>][<a href="https://github.com/GT-STAR-Lab/spec_clip">Code</a>]
              <p></p>
            </td>
          </tr> 
          </table>
          
                                        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="AsymDex" onmouseover="AsymDex">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ff_image'>
                  <img src='images/AsymDex.png' width="180"></div>
                <img src='images/AsymDex.png' width="180">
              </div>
              <script type="text/javascript">
                function superdeep_start() {
                  document.getElementById('ff_image').style.opacity = "1";
                }

                function superdeep_stop() {
                  document.getElementById('ff_image').style.opacity = "0";
                }
                superdeep_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
             <papertitle>AsymDex: Leveraging Asymmetry and Relative Motion in Learning Bimanual Dexterity
                </papertitle>
              </a>
              <br>
	      Zhaodong Yang, <b>Yunhai Han</b>, Harish Ravichandar
              <br>
		<em>Spotlight at RSS Whole-body Control and Bimanual Manipulation Workshop</em>, 2025</em>, <em>Poster at CoRL Whole-body Control and Bimanual Manipulation Workshop</em>, 2024</em>
              <br>
		We present Asymmetric Dexterity (AsymDex), a novel reinforcement learning (RL) framework that can efficiently learn asymmetric bimanual skills for multi-fingered hands without relying on demonstrations, which can be cumbersome to collect. Two crucial ingredients enable AsymDex to reduce the observation and action space dimensions and improve sample efficiency. First, AsymDex leverages the natural asymmetry found in human bimanual manipulation and assigns specific and interdependent roles to each hand: a facilitating hand that moves and reorients the object, and a dominant hand that performs complex manipulations on said object. Second, AsymDex defines and operates over relative observation and action spaces, facilitating responsive coordination between the two hands. Further, AsymDex can be easily integrated with recent advances in grasp learning to handle both the object acquisition phase and the interaction phase of bimanual dexterity. Unlike existing RL-based methods for bimanual dexterity, which are tailored to a specific task, AsymDex can be used to learn a wide variety of bimanual tasks that exhibit asymmetry. Detailed experiments on four simulated asymmetric bimanual dexterous manipulation tasks reveal that AsymDex consistently outperforms strong baselines that challenge its design choices, in terms of success rate and sample efficiency.
              <br>
               [<a href="https://openreview.net/forum?id=g3vjZtYfMb">OpenReview</a>][<a href="https://sites.google.com/view/asymdex-2024/">Webpage</a>]
              <p></p>
            </td>
          </tr> 
          </table>
          
          <strong>&#160 &#160 &#160 2024</strong><br>
          
                        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="CIMER" onmouseover="MimicTouch">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ff_image'>
                  <img src='images/korol.png' width="180"></div>
                <img src='images/korol.png' width="180">
              </div>
              <script type="text/javascript">
                function superdeep_start() {
                  document.getElementById('ff_image').style.opacity = "1";
                }

                function superdeep_stop() {
                  document.getElementById('ff_image').style.opacity = "0";
                }
                superdeep_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
             <papertitle>KOROL: Learning Visualizable Object Feature with Koopman Operator Rollout for Manipulation
                </papertitle>
              </a>
              <br>
	      Hongyi Chen, Abulikemu Abuduweili*, Aviral Agrawal*, <b>Yunhai Han*</b>, Harish Ravichandar, Changliu Liu, and Jeffrey Ichnowski (* Equal contribution)
              <br>
		<em>Accepted by CoRL 2024</em>
              <br>
		Learning dexterous manipulation skills presents significant challenges due to complex nonlinear dynamics that underlie the interactions between objects and multi-fingered hands. Koopman operators have emerged as a robust method for modeling such nonlinear dynamics within a linear framework. However, current methods rely on runtime access to ground-truth (GT) object states, making them unsuitable for vision-based practical applications. Unlike image-to-action policies that implicitly learn visual features for control, we use a dynamics model, specifically the Koopman operator, to learn visually interpretable object features critical for robotic manipulation within a scene. We construct a Koopman operator using object features predicted by a feature extractor and utilize it to auto-regressively advance system states. We train the feature extractor to embed scene information into object features, thereby enabling the accurate propagation of robot trajectories. We evaluate our approach on simulated and real-world robot tasks, with results showing that it outperformed the model-based imitation learning NDP by 1.08× and the image-to-action Diffusion Policy by 1.16×.
              <br>
	 [<a href="https://arxiv.org/abs/2407.00548">arXiv</a>][<a href="https://openreview.net/forum?id=A6ikGJRaKL&noteId=5JYQQNrhkp">OpenReview</a>][<a href="https://github.com/hychen-naza/KOROL">Code</a>]
              <p></p>
            </td>
          </tr> 
          </table>
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="CIMER" onmouseover="MimicTouch">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ff_image'>
                  <img src='images/CIMER.png' width="180"></div>
                <img src='images/CIMER.png' width="180">
              </div>
              <script type="text/javascript">
                function superdeep_start() {
                  document.getElementById('ff_image').style.opacity = "1";
                }

                function superdeep_stop() {
                  document.getElementById('ff_image').style.opacity = "0";
                }
                superdeep_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
             <papertitle>Learning Prehensile Dexterity by Imitating and Emulating State-only Observations
                </papertitle>
              </a>
              <br>
	     <b>Yunhai Han</b>, Zhenyang Chen, Kyle A Williams, and Harish Ravichandar
              <br>
		<em>Accepted by RA-L</em>
              <br>
		When human acquire physical skills (e.g., tennis) from experts, we tend to first learn from merely observing the expert. But this is often insufficient. We then engage in practice, where we try to emulate the expert and ensure that our actions produce similar effects on our environment. Inspired by this observation, we introduce Combining IMitation and Emulation for Motion Refinement (CIMER) -- a two-stage framework to learn dexterous prehensile manipulation skills from state-only observations.  CIMER's first stage involves imitation: simultaneously encode the complex interdependent motions of the robot hand and the object in a structured dynamical system. This results in a reactive motion generation policy that provides a reasonable motion prior, but lacks the ability to reason about contact effects due to the lack of action labels. The second stage involves emulation: learn a motion refinement policy via reinforcement that adjusts the robot hand's motion prior such that the desired object motion is reenacted. CIMER is both task-agnostic (no task-specific reward design or shaping) and intervention-free (no additional teleoperated or labeled demonstrations).
              <br>
	 [<a href="https://arxiv.org/abs/2404.05582">arXiv</a>][<a href="https://sites.google.com/view/cimer-2024/">Webpage</a>][<a href="https://github.com/GT-STAR-Lab/CIMER">Code</a>]
              <p></p>
            </td>
          </tr> 
          </table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="MimicTouch" onmouseover="MimicTouch">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ff_image'>
                  <img src='images/MimicTouch.png' width="180"></div>
                <img src='images/MimicTouch.png' width="180">
              </div>
              <script type="text/javascript">
                function superdeep_start() {
                  document.getElementById('ff_image').style.opacity = "1";
                }

                function superdeep_stop() {
                  document.getElementById('ff_image').style.opacity = "0";
                }
                superdeep_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
             <papertitle>MimicTouch: Leveraging Multi-modal Human Tactile Demonstrations for Contact-rich Manipulation
                </papertitle>
              </a>
              <br>
	     Kelin Yu*, <b>Yunhai Han*</b>, Qixian Wang, Vaibhav Saxena, Danfei Xu, and Ye Zhao (* Equal contribution; Yu is my master student advisee)
	      <br>
<<<<<<< HEAD
             <em>Poster at CoRL Deployable Workshop & NeurIPS Touch Processing Workshop</em>, 2023 
=======
             <em>Accepted by CoRL 2024 & <b>Best Paper</b> at NeurIPS Touch Processing Workshop & Poster at CoRL Deployable Workshop</em>, 2023
>>>>>>> 6ec8003238ff41a225b1fcfca8fdcdc9d16b2cf7
              <br>
		Tactile sensing is critical to fine-grained, contact-rich manipulation tasks, such as insertion and assembly. An effective strategy to learn tactile-guided policy is to train neural networks that map tactile signal to control based on teleoperated demonstration data. However, to provide the teleoperated demonstration, human users often rely on visual feedback to control the robot. This creates a gap between the sensing modality used for controlling the robot (visual) and the modality of interest (tactile). To bridge this gap, we introduce MimicTouch, a novel framework for learning policies directly from demonstrations provided by human users with their hands. The key innovations are i) a human tactile data collection system which collects multi-modal tactile dataset for learning human's tactile-guided control strategy, ii) an imitation learning-based framework for learning human's tactile-guided control strategy through such data, and iii) an online residual RL framework to bridge the embodiment gap between the human hand and the robot gripper. 
              <br>
	 [<a href="https://arxiv.org/abs/2310.16917">arXiv</a>][<a href="https://openreview.net/forum?id=7yMZAUkXa4&noteId=ahxcMFT5Z8">OpenReview</a>][<a href="https://sites.google.com/view/MimicTouch">Webpage</a>]
              <p></p>
            </td>
          </tr> 
          </table>
          <strong>&#160 &#160 &#160 2023</strong><br>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="DexterousManipulation" onmouseover="DexterousManipulation">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ff_image'>
                  <img src='images/dexterous_manipulation.gif' width="180"></div>
                <img src='images/dexterous_manipulation.gif' width="180">
              </div>
              <script type="text/javascript">
                function superdeep_start() {
                  document.getElementById('ff_image').style.opacity = "1";
                }

                function superdeep_stop() {
                  document.getElementById('ff_image').style.opacity = "0";
                }
                superdeep_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
             <papertitle>On the Utility of Koopman Operator Theory in Learning Dexterous Manipulation Skills
                </papertitle>
              </a>
              <br>
	     <b>Yunhai Han</b>, Mandy Xie, Ye Zhao, and Harish Ravichandar
	      <br>
             <em>Accepted by CoRL 2023 (<b>Oral</b>)</em>
              <br>
		This work enables to learn dexterous manipulation skills from a few pieces of demonstration data. Dexterous manipulation is known to be very complex due to the absence of well-studied analytical models and high DoFs. Most existing work addresses this challenge via training a Reinforcement learning (RL) agent with fine-tuned shaped rewards. Motivated by the fact that complex nonlinear dynamics underlie dexterous manipulation and Koopman operators are simple yet powerful control-theoretic structures that help represent complex nonlinear dynamics as linear systems in higher-dimensional spaces, we propose a Koopman operator-based framework (KODex) and demonstrate that it is surprisingly effective for learning dexterous manipulation tasks and offers a number of unique benefits. 
              <br>
	 [<a href="http://arxiv.org/abs/2303.13446">arXiv</a>][<a href="https://openreview.net/forum?id=pw-OTIYrGa">OpenReview</a>][<a href="https://sites.google.com/view/kodex-corl">Webpage</a>][<a href="https://github.com/GT-STAR-Lab/KODex">Code</a>]
              <p></p>
            </td>
          </tr> 
          </table>
	
<strong>&#160 &#160 &#160 2022</strong><br>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="Multi-agent task planning" onmouseover="Multi-agent task planning">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ff_image'>
                  <img src='images/Sim_assisting3.jpg' width="180"></div>
                <img src='images/Sim_assisting3.jpg' width="180">
              </div>
              <script type="text/javascript">
                function keypoint_start() {
                  document.getElementById('ff_image').style.opacity = "1";
                }

                function keypoint_stop() {
                  document.getElementById('ff_image').style.opacity = "0";
                }
                keypoint_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Leveraging Heterogeneous Capabilities in Multi-Agent Systems for Environmental Conflict Resolution
                </papertitle>
              </a>
              <br>
	     Michael E. Cao⋆, Jonas Warnke⋆, <b>Yunhai Han</b>, Xinpei Ni, Ye Zhao, and
Samuel Coogan  (* Equal contributions)
              <br>
        <em>Accepted by SSRR</em>, 2022
              <br>
		This work introduces a high-level controller synthesis framework that enables teams of heterogeneous agents to assist each other in resolving environmental conflicts that appear at runtime. This conflict resolution method is built upon temporal-logic-based reactive
synthesis to guarantee safety and task completion under specific environment assumptions.  Additionally, we implement the proposed framework on a physical multi-agent robotic system
to demonstrate its viability for real world applications.
	      <br>
              [<a href="https://arxiv.org/abs/2206.01833">arXiv</a>][<a href="https://www.youtube.com/watch?v=EKX0g5iJ0Jc&t=2s">Video</a>]
              <p></p>
            </td>
          </tr>
	</table>

<strong>&#160 &#160 &#160 2021</strong><br>
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="auto-calibration" onmouseover="auto-calibration">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ff_image'>
                  <img src='images/Privacy.png' width="180"></div>
                <img src='images/Privacy.png' width="180">
              </div>
              <script type="text/javascript">
                function keypoint_start() {
                  document.getElementById('ff_image').style.opacity = "1";
                }

                function keypoint_stop() {
                  document.getElementById('ff_image').style.opacity = "0";
                }
                keypoint_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>A Numerical Verification Framework for Differential Privacy in Estimation
                </papertitle>
              </a>
              <br>
              <b>Yunhai Han</b> and Sonia Martínez
              <br>
        <em>Accepted by L-CSS and ACC</em>, 2022
              <br>
		This work proposes a numerical method to verify differential privacy in estimation with performance guarantees. To achieve differential privacy, a mechanism (estimator) is
turned into a stochastic mapping; which makes it hard to distinguish outputs produced by close inputs. While obtaining theoretical conditions that guarantee privacy may be possible,
verifying these in practice can be hard.
	      <br>
              [<a href="https://arxiv.org/abs/2108.12094">arXiv</a>][<a href="ACC_pres/ACC 2022.pdf">slides</a>]
              <p></p>
            </td>
          </tr>
	</table>

	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="auto-calibration" onmouseover="auto-calibration">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ff_image'>
                  <img src='images/avs.png' width="180"></div>
                <img src='images/avs.png' width="180">
              </div>
              <script type="text/javascript">
                function keypoint_start() {
                  document.getElementById('ff_image').style.opacity = "1";
                }

                function keypoint_stop() {
                  document.getElementById('ff_image').style.opacity = "0";
                }
                keypoint_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Autonomous Vehicles for Micro-mobility
                </papertitle>
              </a>
              <br>
              Henrik Christensen, David Paz, Hengyuan Zhang, Dominique Meyer, Hao Xiang, <b>Yunhai Han</b>, Yuhan Liu, Andrew Liang, Zheng Zhong, Shiqi Tang
              <br>
        <em>Accepted by Autonomous Intelligent System, Springer</em>, 2021
              <br>
		My teammates at <a href="http://avl.ucsd.edu//">AVL lab</a> and I warpped up the technical details when we explored and developed robust autonomous car systems and architectures for mail-delivery and micro-transit applications on UCSD campus. In this paper, we first tell the details of the initial design experiments and the main lessons/short-comings from the deployments. Then, we present the various approaches to address these challenges. Finally, we outline our future work. My work for this journal paper mainly involves the auto-calibration system for urban autonomous driving.
	      <br>
              [<a href="https://link.springer.com/article/10.1007/s43684-021-00010-2#Abs1">Springer</a>]
              <p></p>
            </td>
          </tr>
	</table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="DifferentialPBD" onmouseover="DifferentialPBD">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ff_image'>
                  <img src='images/fruitpicking.jpg' width="180"></div>
                <img src='images/fruitpicking.jpg' width="180">
              </div>
              <script type="text/javascript">
                function superdeep_start() {
                  document.getElementById('ff_image').style.opacity = "1";
                }

                function superdeep_stop() {
                  document.getElementById('ff_image').style.opacity = "0";
                }
                superdeep_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Learning Generalizable Vision-Tactile Robotic Grasping Strategy for Deformable Objects via Transformer
                </papertitle> 
              </a>
              <br>
              <b>Yunhai Han</b>*, Kelin Yu*, Rahul Batra, Nathan Boyd, Chaitanya Mehta, Tuo Zhao, Yu She, Seth Hutchinson, and Ye Zhao
              <br>
		<em>Accepted by T-MECH</em>
              <br>
		Reliable robotic grasping with deformable objects remains a challenging task due to underactuated contact interactions with a gripper, unknown object dynamics, and variable object geometries. In this study, we propose a Transformer-based robot grasping framework for rigid grippers that leverage tactile information from a GelSight sensor for safe object grasping. The Transformer network learns physical feature embeddings from visual & tactile feedback and predict a final grasp through a multilayer perceptron (MLP) with grasping strength. Using these predictions, the gripper is commanded with an optimal force for safe grasping tasks.
              <br>
	      [<a href="https://arxiv.org/abs/2112.06374">arXiv</a>] [<a href="https://github.com/GTLIDAR/DeformableObjectsGrasping">Webpage</a>] [<a href="https://www.youtube.com/watch?v=W7o8DsTivTk">Video</a>]
              <p></p>
            </td>
          </tr> 
          </table>

	 <strong>&#160 &#160 &#160 2020</strong><br>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="2DSimulation" onmouseover="2DSimulation">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ff_image'>
                  <img src='images/2DSimu.gif' width="180"></div>
                <img src='images/2DSimu.gif' width="180">
              </div>
              <script type="text/javascript">
                function superdeep_start() {
                  document.getElementById('ff_image').style.opacity = "1";
                }

                function superdeep_stop() {
                  document.getElementById('ff_image').style.opacity = "0";
                }
                superdeep_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>A 2D Surgical Simulation Framework for Tool-Tissue Interaction</papertitle>
              </a>
              <br>
              <b>Yunhai Han</b>, Fei Liu and Michael C. Yip
              <br>
        <em>Spotlight presentation at IROS Workshop</em>, 2020 
              <br>
		This framework continuously tracks the motion of manipulator and simulates the tissue  			deformation in presence of collision detection. The deformation energy can be computed for the control and planning 			task.
              <br>
              [<a href="https://arxiv.org/abs/2010.13936">arXiv</a>] [<a href="IROS_workshop/IROS_present.pptx.pdf">slides</a>][<a href="IROS_workshop/a 2d surgical simulation framework_presentation_video.mp4">Talk</a>]
              <p></p>
            </td>
          </tr> 
	  </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="auto-calibration" onmouseover="auto-calibration">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ff_image'>
                  <img src='images/pipeline.png' width="180"></div>
                <img src='images/pipeline.png' width="180">
              </div>
              <script type="text/javascript">
                function keypoint_start() {
                  document.getElementById('ff_image').style.opacity = "1";
                }

                function keypoint_stop() {
                  document.getElementById('ff_image').style.opacity = "0";
                }
                keypoint_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Auto-calibration Method Using Stop Signs for Urban Autonomous Driving Applications
                </papertitle>
              </a>
              <br>
              <b>Yunhai Han</b>*, Yuhan Liu*, David, paz and Henrik Christensen  (* Equal contributions) 
              <br>
        <em>Accepted by ICRA</em>, 2021
              <br>
		For   use   of   cameras   on   an   intelligent   vehicle,driving over a major bump could challenge the 	calibration. It isthen of interest to do dynamic calibration. What structures canbe used for calibration? How about using traffic signs that yourecognize? In this paper an approach is presented for dynamic camera  calibration  based  on  recognition  of  stop  signs.  
	      <br>
              [<a href="https://arxiv.org/abs/2010.07441">arXiv</a>] [<a href="http://avl.ucsd.edu/?page_id=676">Dataset</a>]
              <p></p>
            </td>
          </tr>

          <tr onmouseout="Registration" onmouseover="Registration">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ff_image'>
                  <img src='images/Regis.png' width="180"></div>
                <img src='images/Regis.png' width="180">
              </div>
              <script type="text/javascript">
                function keypoint_start() {
                  document.getElementById('ff_image').style.opacity = "1";
                }

                function keypoint_stop() {
                  document.getElementById('ff_image').style.opacity = "0";
                }
                keypoint_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle><em>Real-to-Sim</em> Registration  of  Deformable  Soft  Tissue  with  Position-Based Dynamics  for  Surgical  Robot  Autonomy
                </papertitle>
              </a>
              <br>
              Fei Liu*, Zihan Li*, <b>Yunhai Han</b>, Jingpei Lu, Florian Richter and Michael C. Yip (* Equal contributions)
              <br>
	      <em>Accepted by ICRA</em>, 2021
              <br>
		Autonomy in robotic surgery is very challenging in unstructured environments, especially when interacting with soft tissues. In this work, we propose an online, continuous, registration method to bridge from 3D visual perception to position-based dynamics modeling of tissues.
	      <br>
              [<a href="https://arxiv.org/abs/2011.00800">arXiv</a>][<a href="ICRA/Demo.mp4">Video</a>] 
              <p></p>
            </td>
          </tr>
	  </table>
        
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Group Projects</heading>
            </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="privacyDifferency" onmouseover="privacyDifferency">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ff_image'>
                  <img src='images/RM.jpg' width="180"></div>
                <img src='images/RM.jpg' width="180">
              </div>
              <script type="text/javascript">
                function superdeep_start() {
                  document.getElementById('ff_image').style.opacity = "1";
                }

                function superdeep_stop() {
                  document.getElementById('ff_image').style.opacity = "0";
                }
                superdeep_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              </a>
              <br><b>RoboMaster</b>
		The RoboMaster program is a platform for robotic competitions and academic exchange founded by Da-Jiang Innovations (DJI). Each team has to design and build a squad of multiple-purpose robots for skirmish combats. I was the vision group leader of YSU Eagle. My group were mainly responsible for the system design of visual components (including object tracking, range estimation and serial port communication) and the PID stability adjustment of the gimbal unit on the mobile tank (to prevent bumps and collisions during movement). 
              <br>
              [<a href="https://www.robomaster.com/en-US">RoboMaster Overview</a>][<a href="https://www.youtube.com/watch?v=S-ds1pzDlG4&feature=youtu.be">Our robots</a>] 
              <p></p>
            </td>
          </tr> 
        </tbody></table>

              <br>
              <br>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Professional Service</heading>
              <p>
              <strong>GT RoboGrads</strong><br>
              Research Vice President<br>
              </p>
              <p>
              <strong>ICRA-2021</strong><br>
              Program Committee Member(Reviewer)<br>
              </p>
	      <p>
              <strong>AIM-2021</strong><br>
              Program Committee Member(Reviewer)<br>
              </p>
              <p>
              <strong>ICRA-2022</strong><br>
              Program Committee Member(Reviewer)<br>
              </p>
              <p>
              <strong>IROS-2022</strong><br>
              Program Committee Member(Reviewer)<br>
              </p>
              <p>
              <strong>ACC-2022</strong><br>
              Program Committee Member(Session Chair)<br>
              </p>
              <p>
              <strong>SSRR-2022</strong><br>
              Program Committee Member(Reviewer)<br>
              </p>
              <p>
              <strong>ICRA-2023</strong><br>
              Program Committee Member(Reviewer)<br>
              </p>
            </td>
            </tr>
        </tbody></table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Teaching Experience</heading>
              <p>
              <strong>MAE145, Robotic Planning & Estimation, UCSD</strong><br>
		21Winter, Teaching Assistant<br>
              </p>
              <p>
              <strong>MAE146, Introduction to ML Algorithms, UCSD</strong><br>
		21Spring, Teaching Assistant<br>
              </p>
            </td>
            </tr>
        </tbody></table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Working Experience</heading>
              <p>
              <strong>Research Assistant, Georgia Institute of Technology</strong><br>
		21Summer ~ 22Spring<br>
              </p>
            </td>
            </tr>
        </tbody></table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Misc</heading>
              <p>
              <strong>Running</strong>: Once finished half-marathon (21km) within 2 hours <br>
              <strong>Soccer and Badminton</strong><br>
               <strong>Photograpy  </strong><br>
                <strong>Gaming</strong>: League of Legend<br> 
                <strong>Anime, Manga and Movie</strong><br>
              </p>
            </td>
            </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Template from this handsome guy.</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
